{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1: Posterior inference\n",
    "\n",
    "Since we have a Beta(4,4) prior for $\\theta$, we have that:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta) \\propto \\theta^3\\cdot (1-\\theta)^3\n",
    "\\end{equation*}\n",
    "\n",
    "Now we are told that $n=10$ and $Y<3$, therefore:\n",
    "\n",
    "\\begin{equation*}\n",
    "Pr(Y<3 \\mid \\theta) = \\sum\\limits_{y=0}^{2} p(Y=y \\mid \\theta) = {10 \\choose 0} (1-\\theta)^{10} + {10 \\choose 1} \\theta (1-\\theta)^9 + {10 \\choose 2} \\theta^2 (1-\\theta)^8 \n",
    "\\end{equation*}\n",
    "\n",
    "Hence by Bayes' theorem:\n",
    "\n",
    "\\begin{equation*}\n",
    "Pr(\\theta \\mid Y<3) \\propto Pr(Y<3 \\mid \\theta) \\cdot Pr(\\theta) = \\theta^3 (1-\\theta)^{13} + 10 \\cdot \\theta^4 (1-\\theta)^{12} + 45\\cdot \\theta^5 (1-\\theta)^{11} \n",
    "\\end{equation*}\n",
    "\n",
    "Sketch of the distribution:\n",
    "![title](figures/fig2.1.png)\n",
    "\n",
    "---\n",
    "Python code to generate the distribution graph:\n",
    "```python\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "theta = np.linspace(0,1, 100)\n",
    "get_p_theta = lambda t: t**3 * (1-t)**13 + 10 * t**4 * (1-t)**12 + 45 * t**5 * (1-t)**11\n",
    "p_theta = get_p_theta(theta)\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Scatter(\n",
    "            x=theta,\n",
    "            y=p_theta/np.sum(p_theta)\n",
    "        )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={'text': 'Fig 2.1 - Posterior distribution density of θ',\n",
    "           'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n",
    "    xaxis={'title': 'θ'},\n",
    "    yaxis={'title': 'p(θ | y)'})\n",
    "\n",
    "fig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2: Predictive distributions\n",
    "\n",
    "**Notation** Let $T$ be the event of resulting in tail, let $N$ be the random variable denoting the number of additional spins until head shows up, let $p_i = Pr(H \\mid C_i)$ for $i=1,2$. We want to compute $E\\left[ N \\mid TT \\right]$ where $TT$ denotes the event that the first two spins from the chosen coin are tails.\n",
    "\n",
    "By the law of total probabilities we have:\n",
    "\n",
    "\\begin{equation}\n",
    "p(N=n\\mid TT) = \\sum\\limits_{i=1,2} p(N=n, C=C_i \\mid TT) = \\sum\\limits_{i=1,2} p(N=n \\mid TT, C=C_i)\\cdot p(C=C_i \\mid TT)\n",
    "\\end{equation}\n",
    "\n",
    "Let's compute both factors of equation (1):\n",
    "\n",
    "\\begin{equation*}\n",
    "p(N=n \\mid TT, C=C_i) = p(N=n \\mid C=C_i) = p_i\\cdot(1-p_i)^{n-1} \\:\\: \\text{for } i=1,2\n",
    "\\end{equation*}\n",
    "and \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "p(C=C_i  \\mid TT ) &  = \\frac{ p(TT\\mid C=C_i) \\cdot p(C=C_i) }{\\sum\\limits_{j=1,2} p(TT\\mid C=C_j) \\cdot p(C=C_j)} = \\frac{p(T\\mid C=C_i)^2 \\cdot p(C=C_i)}{\\sum\\limits_{j=1,2} p(T\\mid C=C_j)^2 \\cdot p(C=C_j)} \\\\[5pt]\n",
    "&  =\\begin{cases} \\frac{\\left(\\frac{2}{5}\\right)^2 \\cdot \\frac{1}{2}}{ \\left(\\frac{2}{5}\\right)^2 \\cdot \\frac{1}{2} + \\left(\\frac{3}{5}\\right)^2 \\cdot \\frac{1}{2} }\\\\\n",
    "    \\frac{\\left(\\frac{3}{5}\\right)^2 \\cdot \\frac{1}{2}}{ \\left(\\frac{2}{5}\\right)^2 \\cdot \\frac{1}{2} + \\left(\\frac{3}{5}\\right)^2 \\cdot \\frac{1}{2} }\n",
    "    \\end{cases}\n",
    "=\\begin{cases} \\frac{4}{13} \\text{ if } i=1 \\\\\n",
    "    \\frac{9}{13} \\text{ if } i=2\n",
    "    \\end{cases}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "Combining the results and recalling the geometric series equality: $\\sum_{n=1}^{\\infty}nq^{n-1}=\\dfrac{1}{(1-q)^2}$ for $q\\in (0,1) $ we obtain:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "E[N\\mid TT] & = \\sum_{n=1}^{\\infty} n \\cdot p(N=n\\mid TT) = \\\\[5pt]\n",
    "& = \\sum_{n=1}^{\\infty} n \\cdot \\left( \\frac{4}{13} p_1 (1-p_1)^{n-1} + \\frac{9}{13} p_2 (1-p_2)^{n-1}  \\right)\\\\[5pt]\n",
    "& = \\frac{4 p_1}{13} \\cdot \\sum_{n=1}^{\\infty} n (1-p_1)^{n-1} + \\frac{9p_2}{13} \\cdot \\sum_{n=1}^{\\infty} n (1-p_2)^{n-1}\\\\[5pt]\n",
    "& = \\frac{4 p1}{13} \\cdot \\frac{1}{p_1^2} + \\frac{9 p_2}{13} \\cdot \\frac{1}{p_2^2}\\\\[5pt]\n",
    "& = \\frac{4}{13} \\cdot \\frac{1}{p_1} + \\frac{9}{13}\\cdot \\frac{1}{p_2} \\approx 2.24\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.3: Predictive distributions\n",
    "\n",
    "(a) We have $y \\sim Bin\\left(n=1000, p=\\frac{1}{6}\\right)$, hence \n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "& E[y] = np = 166.7 \\\\\n",
    "& var(y) = np(1-p) = 138.9 \\approx 11.8^2\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore the normal approximation for $y$ is $N(166.7, 11.8^2)$ with distribution:\n",
    "\n",
    "![title](figures/fig2.2.png)\n",
    "\n",
    "(b) Using [scipy.stats.norm.ppf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) function: \n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "v = stats.norm(loc=166.7, scale=11.8)\n",
    "y = v.pdf(x)\n",
    "\n",
    "print(np.rint(v.ppf([0.05, 0.25, 0.5, 0.75, 0.95])))\n",
    "\n",
    "array([147., 159., 167., 175., 186.])\n",
    "```\n",
    "Notice that we round to the nearest integer since $y$ is an integer valued random variable.\n",
    "\n",
    "---\n",
    "\n",
    "Python code to generate the distribution in point (a):\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "v = stats.norm(loc=166.7, scale=11.8)\n",
    "x = np.linspace(100, 250, 300)\n",
    "y = v.pdf(x)\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y\n",
    "        )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={'text': 'Fig 2.2 - Approx distribution for y',\n",
    "           'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n",
    "    xaxis={'title': 'y'}, yaxis={'title': 'p(y)'})\n",
    "\n",
    "fig\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.4: Predictive distributions\n",
    "\n",
    "We want to approximate $y\\mid \\theta \\sim N(\\mu, \\sigma^2)$.\n",
    "\n",
    "We have: \n",
    "\\begin{equation*}\n",
    "\\mu = E[y\\mid \\theta] = n \\theta = \n",
    "\\begin{cases}\n",
    "    1000\\frac{1}{12} \\\\\n",
    "    1000\\frac{1}{4} \\\\\n",
    "    1000\\frac{1}{6} \\\\\n",
    "\\end{cases}\n",
    "= \\begin{cases}\n",
    "    83.3 \\:\\text{ for } \\theta=\\frac{1}{2}   \\\\\n",
    "    250.0 \\text{ for } \\theta= \\frac{1}{4} \\\\\n",
    "    166.7 \\text{ for } \\theta=\\frac{1}{6} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "and \n",
    "\\begin{equation*}\n",
    "\\sigma^2 = n \\theta (1-\\theta)= \n",
    "\\begin{cases}\n",
    "    1000\\frac{1}{12} \\frac{11}{12} \\\\\n",
    "    1000\\frac{1}{4} \\frac{3}{4} \\\\\n",
    "    1000\\frac{1}{6} \\frac{5}{6}\\\\\n",
    "\\end{cases}\n",
    "= \\begin{cases}\n",
    "    8.7^2 \\text{ for } \\theta=\\frac{1}{2}   \\\\\n",
    "    13.7^2 \\text{ for } \\theta= \\frac{1}{4} \\\\\n",
    "    11.8^2 \\text{ for } \\theta=\\frac{1}{6} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore using conditional normal approximation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "p(y) & = \\sum_\\theta p(y, \\theta) = \\sum_\\theta p(y \\mid \\theta) p(\\theta)\\\\[5pt]\n",
    "& \\approx 0.25 \\cdot N(83.3, 8.7^2) + 0.5\\cdot N(166.7, 11.8^2) + 0.25\\cdot N(250, 13.7^2)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Resulting in a multimodal distribution:\n",
    "![title](figures/fig2.3.png)\n",
    "\n",
    "\n",
    "(b) To approximate quantiles:\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "v = stats.norm(loc=[83.3, 166.7, 250], scale=[8.7, 11.8, 13.7])\n",
    "x = np.linspace(0, 350, 10000)[:, None]\n",
    "\n",
    "weights = np.array([0.25, 0.5, 0.25]).reshape(1,3)\n",
    "y = np.sum(v.pdf(x)/v.pdf(x).sum(axis=0) * weights, axis=1)\n",
    "\n",
    "quantiles = np.array([.05, .25, .50, .75, .95])\n",
    "for q in quantiles:\n",
    "    \n",
    "    print(q, np.rint(np.mean(x[np.isclose(y.cumsum(), q, rtol = x[1])])))\n",
    "    \n",
    "0.05 76.0\n",
    "0.25 120.0\n",
    "0.50 167.0\n",
    "0.75 209.0\n",
    "0.95 263.0\n",
    "```\n",
    "As before we round to nearest integer since $y$ is discrete.\n",
    "\n",
    "---\n",
    "\n",
    "Python code to generate the distribution in point (a):\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "v = stats.norm(loc=[83.3, 166.7, 250], scale=[8.7, 11.8, 13.7])\n",
    "x = np.linspace(0, 350, 10000)[:, None]\n",
    "\n",
    "weights = np.array([0.25, 0.5, 0.25]).reshape(1,3)\n",
    "y = np.sum(v.pdf(x)/v.pdf(x).sum(axis=0) * weights, axis=1)\n",
    "\n",
    "fig = go.Figure(\n",
    "        go.Scatter(\n",
    "            x=x.squeeze(),\n",
    "            y=y\n",
    "        )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={'text': 'Fig 2.3 - Approx distribution for y',\n",
    "           'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n",
    "    xaxis={'title': 'y'}, yaxis={'title': 'p(y)'})\n",
    "\n",
    "fig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exercise 2.5: Posterior distribution as a compromise between prior information and data\n",
    " \n",
    "(a) For each $k = 0, 1,\\dots , n$, we have\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "Pr(y = k) & = \\int^1_0 Pr(y = k\\mid \\theta)d\\theta \\\\[5pt]\n",
    "& = \\int^1_0 {n \\choose k} \\theta^k (1-\\theta)^{n-k} d\\theta \\\\[5pt]\n",
    "& = {n \\choose k} \\cdot \\frac{\\Gamma(k+1) \\Gamma(n-k+1)}{\\Gamma(n+2)}\\\\[5pt]\n",
    "& = \\frac{n! k! (n-k)!}{k!(n-k)!(n+1)!} = \\frac{1}{n+1}\\\\[5pt]\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "(b) Choosing a $Beta(\\alpha, \\beta)$ prior for $\\theta$ leads to the following beta posterior distribution: \n",
    "\n",
    "\\begin{equation*}\n",
    "    p(\\theta \\mid y) \\propto Beta(\\theta \\mid \\alpha + y, \\beta + n -y) \n",
    "\\end{equation*}\n",
    "\n",
    "which has (posterior) mean \n",
    "\\begin{equation*}\n",
    "    E[\\theta \\mid y]= \\frac{\\alpha + y}{\\alpha + \\beta + n}\n",
    "\\end{equation*}\n",
    "\n",
    "Now notice that \n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "E[\\theta \\mid y] & =  \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\cdot E[\\theta] + \\frac{n}{\\alpha + \\beta + n}  \\cdot \\frac{y}{n}\\\\[5pt]\n",
    "& = w_1 \\cdot E[\\theta]  + (1-w_1)  \\cdot \\frac{y}{n}  \\\\\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "i.e. $E[\\theta \\mid y]$ is a convex combination of $\\frac{\\alpha}{\\alpha + \\beta}$ and $\\frac{y}{n}$, therefore it lies in between these two values.\n",
    "\n",
    "(c) Let $\\theta \\sim Unif[0,1] = Beta(1,1)$, thus has variance $var(\\theta) = \\frac{1}{12}$. \n",
    "Given the data $y$, $n$, the posterior variance is \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "var(\\theta \\mid y) & = \\frac{(1+y)(1+n-y)}{(1+y+1+n-y)^2(1+y+1+n-y+1)}\\\\[5pt]\n",
    "& = \\frac{(1+y)(1+n-y)}{(2+n)^2 (3+n)}\\\\[5pt]\n",
    "& = \\left(\\frac{1+y}{2+n}\\right) \\cdot \\left(\\frac{1+n-y}{2+n}\\right) \\cdot \\left(\\frac{1}{3+n}\\right)\\\\[5pt]\n",
    "& < \\frac{1}{4} \\cdot \\frac{1}{3} = \\frac{1}{12}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Since the first two factors $\\left(\\frac{1+y}{2+n}\\right), \\left(\\frac{1+n-y}{2+n}\\right)$ are in $(0,1)$ and sum to 1, therefore their product is less than or equal to $\\frac{1}{4}$. The third factor is less than $\\frac{1}{3} \\:\\: \\forall n>0$.\n",
    "\n",
    "(d) Let $\\theta \\sim Beta(\\alpha, \\beta)$ for some $\\alpha, \\beta >0$, its prior and posterio variances are \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "& var(\\theta) = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha + \\beta + 1)}\\\\[5pt]\n",
    "& var(\\theta\\mid y) = \\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha + \\beta + n +1)}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Trying a brute force (terrible) python loop we can get few values:\n",
    "\n",
    "|    |   a |   b |   n |   y |\n",
    "|---:|----:|----:|----:|----:|\n",
    "|  0 |   1 |   5 |   2 |   1 |\n",
    "|  1 |   1 |   5 |   3 |   2 |\n",
    "|  2 |   1 |   5 |   4 |   3 |\n",
    "|  3 |   1 |   5 |   5 |   4 |\n",
    "|  4 |   3 |   1 |   1 |   0 |\n",
    "|  5 |   4 |   1 |   1 |   0 |\n",
    "...\n",
    "| 14 |   5 |   1 |   4 |   1 |\n",
    "| 15 |   5 |   1 |   5 |   0 |\n",
    "| 16 |   5 |   1 |   5 |   1 |\n",
    "| 17 |   5 |   2 |   1 |   0 |\n",
    "\n",
    "Python code\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "prior_var = lambda a,b: a*b/((a+b)**2 * (a+b+1))      \n",
    "post_var = lambda a,b, n,y: (a+y)*(b+n-y)/((a+b+n)**2 *(a+b+n+1))   \n",
    "\n",
    "df = pd.DataFrame(columns=list('abny'))\n",
    "cnt = 0\n",
    "for a in range(1, 6):\n",
    "    for b in range(1, 6):\n",
    "        prior = prior_var(a,b)\n",
    "        for n in range(1, 6):\n",
    "            for y in range(n):\n",
    "                post = post_var(a,b,n,y)\n",
    "                if prior < post:\n",
    "                    df.loc[cnt] = (a,b,n,y)\n",
    "                    cnt+=1\n",
    "                    \n",
    "print(df.to_markdown())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.7: Noninformative prior densities:\n",
    "\n",
    "(a) First and foremost let's deduce the natural parameter expressing $p(y\\mid \\theta)$ in an exponential form:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "p(y \\mid \\theta) & = {n \\choose y} \\theta^y (1-\\theta)^{n-y} \\\\[5pt]\n",
    "& = {n \\choose y} \\text{exp}\\Big( y \\text{log}(\\theta) + (n-y) \\text{log}(1-\\theta)\\Big) \\\\[5pt]\n",
    "& = {n \\choose y} \\text{exp}\\Big( \\text{log}\\left(\\frac{\\theta}{1-\\theta}\\right) \\cdot y \\Big) \\cdot \\text{exp} \\Big( n \\text{log}(1-\\theta)\\Big) \\\\[5pt]\n",
    "& = {n \\choose y} (1-\\theta)^n \\text{exp}\\Big( \\text{log}\\left(\\frac{\\theta}{1-\\theta}\\right) \\cdot y \\Big) \\\\[5pt]\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Hence we have $f(y) = {n \\choose y}$, $g(\\theta) = (1-\\theta)$, $t(y) = y$ and the natural parameter is $\\phi(\\theta) = \\text{log}\\left( \\frac{\\theta}{1-\\theta}\\right)$.\n",
    "\n",
    "Now assume that the prior for $\\phi$ is uniform i.e. $p(\\phi) \\propto 1$, then using the variable change to $\\theta$ we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "p(\\theta) & = p(\\phi) \\Big| \\frac{d\\phi}{d\\theta}\\Big| \\propto \\frac{d \\text{log}\\left( \\frac{\\theta}{1-\\theta}\\right)}{ d\\theta}\\\\[5pt]\n",
    "& = \\frac{d \\Big(\\text{log}(\\theta) - \\text{log}(1-\\theta) \\Big)}{ d\\theta} \\\\[5pt]\n",
    "& = \\frac{1}{\\theta} + \\frac{1}{1-\\theta} = \\theta^{-1} \\cdot (1-\\theta)^{-1}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "(b) For the posterior of $\\theta$ we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta\\mid y) \\propto p(y \\mid \\theta) p(\\theta) = {n \\choose y} \\theta^{y-1} (1-\\theta)^{n-y-1}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore for the two cases we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta\\mid y) \\propto \n",
    "\\begin{cases}\n",
    "\\theta^{-1} (1-\\theta)^{n-1} \\text{ for } y=0 \\\\ \n",
    "\\theta^{n-1} (1-\\theta)^{-1} \\text{ for } y=n \n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "which is not integrable near $0$, $1$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.8: Normal distribution with unknown mean\n",
    "\n",
    "We have $\\bar{y} = 150$, $y_i \\sim N(\\theta, \\sigma^2 = 20^2)$ and $\\theta \\sim N(\\mu_0 = 180, \\tau^2_0 = 40^2)$ \n",
    "\n",
    "(a) As developed in the chapter theory $\\theta \\mid y \\sim N(\\mu_1, \\tau^2_1)$ with:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\mu_1 & = \\frac{ \\frac{1}{\\tau^2_0} \\mu_0 + \\frac{n}{\\sigma^2} \\bar{y}}{\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}} =  \\frac{ \\mu_0\\sigma^2 + n\\tau^2_0\\bar{y}}{\\sigma^2 +  \\tau^2_0 n } = \\frac{180\\cdot 20^2 + 150 \\cdot 40^2 n}{20^2 + 40^2 n}\\\\[5pt]\n",
    "\\tau^2_1 & = \\frac{1}{\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}} = \\frac{ \\sigma^2 + \\tau^2_0}{\\sigma^2 +  \\tau^2_0 n} = \\frac{20^2 \\cdot 40^2}{20^2 + 40^2 n}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "(b) Let $\\tilde{y}$ be a new observation, then as developed in the chapter theory:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\tilde{y} \\mid y \\sim N(\\mu_1, \\sigma^2+\\tau^2_1) \n",
    "\\end{equation*}\n",
    "\n",
    "with $\\mu_1$, $\\sigma^2$ and $\\tau^2_1$ as above.\n",
    "\n",
    "(c) & (d) Let's use python to get both answers:\n",
    "\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def get_params(n):\n",
    "    \"\"\"\n",
    "    Returns mu and tau^2 based on n\n",
    "    \"\"\"\n",
    "    \n",
    "    d = (20**2 + 40**2 * n)\n",
    "    mu = (180* 20**2 + 150 * 40**2 * n)/d\n",
    "    tau2 = (20*40)**2/d\n",
    "    return mu, tau2\n",
    "\n",
    "for n in [10, 100]:\n",
    "    \n",
    "    mu, t2 = get_params(n=n)\n",
    "    theta_dist = stats.norm(loc=mu, scale=np.sqrt(t2))\n",
    "    y_dist = stats.norm(loc=mu, scale=np.sqrt(t2 + 20**2))\n",
    "    \n",
    "    print(f'n={n}', '-'*5,\n",
    "          f'95% posterior interval for θ: {theta_dist.ppf([0.025, 0.975]).round(2).tolist()}', \n",
    "          f'95% posterior interval for y: {y_dist.ppf([0.025, 0.975]).round(2).tolist()}',\n",
    "          '\\n',\n",
    "          sep='\\n')\n",
    "          \n",
    "n=10\n",
    "-----\n",
    "95% posterior interval for θ: [138.49, 162.98]\n",
    "95% posterior interval for y: [109.66, 191.8]\n",
    "\n",
    "\n",
    "n=100\n",
    "-----\n",
    "95% posterior interval for θ: [146.16, 153.99]\n",
    "95% posterior interval for y: [110.68, 189.47]\n",
    "```\n",
    "\n",
    "Remark that the posterior is for $\\tilde{y}$, I just couldn't manage to write it in python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
